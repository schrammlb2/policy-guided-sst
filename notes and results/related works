related works

    Anytime sampling-based motion planners such as RRT \citep{RRT}, RRT$^*$ \citep{RRTStar}, and SST \citep{SST} have become popular in recent years due to their efficient exploration and relatively good performance in higher dimensions. These methods construct a search tree by randomly sampling nodes in the tree to expand, expanding them with random controls, and removing nodes from the tree that have been obsoleted by new paths. These methods are capable of quickly finding solutions to difficult planning problems and can be extended easily to high-dimensional planning problems. Additionally, SST in particular comes with a number of theoretical guarantees, such as and asymptotic $\delta$-robust completeness and asymptotic $\delta$-robust near-optimality. However, its performance deteriorates quickly in higher dimensions, as its worst-case performance is exponential in both the dimensionality of the state-space and the action space. 

    Sampling-based planning methods often attempt to minimize their search time through the use of heuristics. Several methods, such as Informed RRT* \citep{InformedRRTStar}, BIT* \citep{BIT}, and RABIT* \citep{RABIT} use heuristics to select where points in the space should be sampled from, in order to reduce the size of the search space. However, this kind of selective sampling is not always possible for problems with more complex dynamics. The geometric bound used by these methods (sampling only within the ellipse where it is possible for the optimal path to lie) is only usable when there is a known lower bound on the cost between two points which is a constant multiple of euclidean distance. This bound may not be known for arbitrary dynamics, and may form only a loose bound when it is known
    
    Another common approach to solving optimal motion problems is reinforcement learning. Reinforcement learning can solve many optimal motion planning problems, such as grasping \citep{HER} and navigation \cite{surmann_deep_2020}. A major advantage of this approach is that, although it is computing-intensive to train, it requires little time to run once trained. It also tends to scale better to high-dimensional problems than planning, as evidenced by its success at extremely high-dimensional problems such as Atari games \citep{DQN}. A number of recent methods have also explored variations of model-free RL that behave similarly to model-based planning methods. Temporal difference models predict the displace vector from the goal state after $t$ steps, and train a policy to minimize the norm of that displacement vector \citep{TDM}. Universal planning networks learn a dynamics model for planning in a latent space, then optimize their actions directly by performing gradient descent on the cumulative cost of the action sequence \citep{UPM}. 

    
    RL-RRT attempts to improve the performance of sampling-based motion planning by biasing the sampled control sequences toward ones that avoid collisions \citep{RLRRT}. It does this by first training an obstacle-aware policy and a time-to-reach function that is used to estimate whether a given sampled point is reachable. The algorithm then samples points until it finds one that is reachable by the policy, and uses the policy to sample a control sequence that reaches that point. This approach significantly improves both the planning time and path quality of discovered paths compared to SST. However, these changes are primarily useful for planning problems concerned with obstacle avoidance, and may be less relevant to problems as grasping, where obstacle avoidance may be less important than efficiently navigating a high-dimensional space. As described in the original paper, RL-RRT may have difficulty navigating high-dimensional spaces due to the fact that the proportion of points close to any given point in state space decreases exponentially in the number of dimensions, which may lead to RL-RRT rejecting sampled points for a long time before finding an acceptable expansion.

    -----------------------------------------------------------------------------------------------------------------------------------------


    Steering  function-based  kinodynamic  planners,  such  askinodynamic RRT* [26] and D-FMT [23] rely on a steeringfunction to “pull” the tree to achieve rapid exploration [21]and a proper distance function [26], [19], [27]. RL-RRT usesAutoRL  [4]  to  learn  steering  functions,  thus  bypassing  thechallenging two-point boundary value problem.Steering function free-based approaches, such as EST [21]and  SST  [14],  propagate  random  actions  from  a  selectednode.  These  methods  can  be  applied  to  a  variety  of  robotdynamics, although they tend to “wander” [1], thus they cantake a long time to identify a solution.Recent  research  has  offered  several  solutions  for  P2Pobstacle-avoidance policies on a differential drive robot fromraw  sensory  input,  including  learning  from  demonstration[20],  curriculum  learning  [28],  and  reinforcement  learning[25],  [4].  Other  research  offers  hierarchical  solutions  tonavigation,  where  the  RL  agent  executes  a  path  identifiedby another planner, e.g., from a grid [5], PRMs [6], [8], ormanually  selected  waypoints  [11].  However,  none  of  thosemethods  are  designed  for  kinodynamic  robots,  leading  tofailures at milestones due to dynamic constraints [8].Designing  an  effective  distance  function  for  sampling-based  kinodynamnic  motion  planning  is  challenging  [19].The   commonly   used   Euclidean   and   weighted   Euclideandistance  for  configuration  space  planning  is  inefficient  askinodynamic robot states have limited reachability [13]. The minimum TTR between states is a highly effective distancefunction [19], [27] but is often too computationally-expensive to  be  used  as  a  distance  function  [19].  While  learned  TTR of a near-optimal differential drive steering function [19] can overcome  the  computational  complexity,  this  approach  still requires  a  near-optimal  steering  function.  Indirect  optimal control  has  also  been  used  to  generate  training  samplescomposed  of  minimum  TTR  and  optimal  control  actionsalong trajectories [27]. However, this approach currently onlyworks for low dimensional systems such as inverted pendu-lum and does not handle limited action bounds. Our approachaddresses  these  challenges  by  bypassing  the  necessity  ofa  near-optimal  steering  function  via  RL.  Unlike  previous
methods, we also take into account obstacle avoidance, whichcan significantly change the minimum TTR


Related works
	Planning
		Kinodynamic, sampling based
			Kinodynamic RRT
			SST 
		Heuristics
			Informed RRT*
			BIT/RABIT

			Heuristics are often necessary to inform search
				Can dramatically reduce search time
			However, many sampling-based planning algorithms make strong assumptions about the dynamics of the problems they attempt to solve
				These methods only work for geometric problems, and for many reasons it is desirable to have planning algorithms that work on arbitrary kinodynamic problems

				Gradient-descent sampling has a few major benefits
					1) Unlike rejection sampling, the convergence rate of gradient descent is independant of the dimension, making this approach suitable to high-dimensional spaces
					2) This method works for arbitrary dynamics, allowing us to efficiently restrict the search space even in problems with complex dynamics.  
					3) In systems where sampling viable goal states is non-trivial (eg, grasping, which would require predicting viable grips), gradient descent sampling allows us to sample goal states by maximizing the value of a state
